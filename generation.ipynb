{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c014cd14-5cb8-4515-b919-6852bf6c1e5e",
   "metadata": {},
   "source": [
    "# AudioGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a908c0-96c8-4a10-b164-94f83f086a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecca/.conda/envs/song_env/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 8.94 MiB is free. Including non-PyTorch memory, this process has 3.62 GiB memory in use. Of the allocated memory 3.48 GiB is allocated by PyTorch, and 77.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maudiocraft\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioGen\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAudioGen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfacebook/audiogen-medium\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/song_env/lib/python3.9/site-packages/audiocraft/models/audiogen.py:96\u001b[0m, in \u001b[0;36mAudioGen.get_pretrained\u001b[0;34m(name, device)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AudioGen(name, compression_model, lm, max_duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     95\u001b[0m compression_model \u001b[38;5;241m=\u001b[39m load_compression_model(name, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 96\u001b[0m lm \u001b[38;5;241m=\u001b[39m \u001b[43mload_lm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_wav\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m lm\u001b[38;5;241m.\u001b[39mcondition_provider\u001b[38;5;241m.\u001b[39mconditioners, \\\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudioGen do not support waveform conditioning for now\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m AudioGen(name, compression_model, lm)\n",
      "File \u001b[0;32m~/.conda/envs/song_env/lib/python3.9/site-packages/audiocraft/models/loaders.py:114\u001b[0m, in \u001b[0;36mload_lm_model\u001b[0;34m(file_or_url_or_id, device, cache_dir)\u001b[0m\n\u001b[1;32m    112\u001b[0m _delete_param(cfg, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconditioners.args.merge_text_conditions_p\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    113\u001b[0m _delete_param(cfg, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconditioners.args.drop_desc_p\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuilders\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_lm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(pkg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_state\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    116\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m~/.conda/envs/song_env/lib/python3.9/site-packages/audiocraft/models/builders.py:107\u001b[0m, in \u001b[0;36mget_lm_model\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m    103\u001b[0m         codebooks_pattern_cfg \u001b[38;5;241m=\u001b[39m omegaconf\u001b[38;5;241m.\u001b[39mOmegaConf\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m    104\u001b[0m             {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodeling\u001b[39m\u001b[38;5;124m'\u001b[39m: q_modeling, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelay\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelays\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(n_q))}}\n\u001b[1;32m    105\u001b[0m         )\n\u001b[1;32m    106\u001b[0m     pattern_provider \u001b[38;5;241m=\u001b[39m get_codebooks_pattern_provider(n_q, codebooks_pattern_cfg)\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLMModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpattern_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpattern_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcondition_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcondition_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfuser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcfg_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattribute_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattribute_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(cfg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected LM model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mlm_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/song_env/lib/python3.9/site-packages/audiocraft/models/lm.py:167\u001b[0m, in \u001b[0;36mLMModel.__init__\u001b[0;34m(self, pattern_provider, condition_provider, fuser, n_q, card, dim, num_heads, hidden_scale, norm, norm_first, emb_lr, bias_proj, weight_init, depthwise_init, zero_bias_init, cfg_dropout, cfg_coef, attribute_dropout, two_step_cfg, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    166\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m get_activation_fn(kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivation\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m \u001b[43mStreamingTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_feedforward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhidden_scale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_norm: tp\u001b[38;5;241m.\u001b[39mOptional[nn\u001b[38;5;241m.\u001b[39mModule] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm_first:\n",
      "File \u001b[0;32m~/.conda/envs/song_env/lib/python3.9/site-packages/audiocraft/modules/transformer.py:638\u001b[0m, in \u001b[0;36mStreamingTransformer.__init__\u001b[0;34m(self, d_model, num_heads, num_layers, dim_feedforward, dropout, bias_ff, bias_attn, causal, past_context, custom, memory_efficient, attention_as_float32, cross_attention, layer_scale, positional_embedding, max_period, positional_scale, xpos, lr, weight_decay, layer_class, checkpointing, device, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers):\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 638\u001b[0m         \u001b[43mlayer_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m            \u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_feedforward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim_feedforward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_ff\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_ff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmemory_efficient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_efficient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_as_float32\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_as_float32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcross_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpointing \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    648\u001b[0m         \u001b[38;5;66;03m# see audiocraft/optim/fsdp.py, magic signal to indicate this requires fixing the\u001b[39;00m\n\u001b[1;32m    649\u001b[0m         \u001b[38;5;66;03m# backward hook inside of FSDP...\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/song_env/lib/python3.9/site-packages/audiocraft/modules/transformer.py:486\u001b[0m, in \u001b[0;36mStreamingTransformerLayer.__init__\u001b[0;34m(self, d_model, num_heads, dim_feedforward, dropout, bias_ff, bias_attn, causal, past_context, custom, memory_efficient, attention_as_float32, qk_layer_norm, qk_layer_norm_cross, cross_attention, layer_scale, rope, attention_dropout, kv_repeat, norm, device, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_model: \u001b[38;5;28mint\u001b[39m, num_heads: \u001b[38;5;28mint\u001b[39m, dim_feedforward: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, dropout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m    479\u001b[0m              bias_ff: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, bias_attn: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    480\u001b[0m              past_context: tp\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, custom: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m              rope: tp\u001b[38;5;241m.\u001b[39mOptional[RotaryEmbedding] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, attention_dropout: tp\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    485\u001b[0m              kv_repeat: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, norm: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlayer_norm\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 486\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_feedforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m     factory_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: device, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m'\u001b[39m: dtype}\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;66;03m# Redefine self_attn to our streaming multi-head attention\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/song_env/lib/python3.9/site-packages/torch/nn/modules/transformer.py:739\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.__init__\u001b[0;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, bias, device, dtype)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1 \u001b[38;5;241m=\u001b[39m Linear(d_model, dim_feedforward, bias\u001b[38;5;241m=\u001b[39mbias, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m Dropout(dropout)\n\u001b[0;32m--> 739\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2 \u001b[38;5;241m=\u001b[39m \u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim_feedforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first \u001b[38;5;241m=\u001b[39m norm_first\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1 \u001b[38;5;241m=\u001b[39m LayerNorm(d_model, eps\u001b[38;5;241m=\u001b[39mlayer_norm_eps, bias\u001b[38;5;241m=\u001b[39mbias, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[0;32m~/.conda/envs/song_env/lib/python3.9/site-packages/torch/nn/modules/linear.py:106\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[0;32m--> 106\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 3.63 GiB of which 8.94 MiB is free. Including non-PyTorch memory, this process has 3.62 GiB memory in use. Of the allocated memory 3.48 GiB is allocated by PyTorch, and 77.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from audiocraft.models import AudioGen\n",
    "\n",
    "model = AudioGen.get_pretrained('facebook/audiogen-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2666df-f8d0-46e5-99f9-ad70ba842668",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mset_generation_params(\n\u001b[1;32m      2\u001b[0m     use_sampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      3\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m,\n\u001b[1;32m      4\u001b[0m     duration\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b138165b-07f1-4ee6-8879-e4c46d79d7ef",
   "metadata": {},
   "source": [
    "## Audio Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07fec0e-bc9a-4c77-b88e-ef856f031b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torchaudio\n",
    "import torch\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "def get_bip_bip(bip_duration=0.125, frequency=440,\n",
    "                duration=0.5, sample_rate=16000, device=\"cuda\"):\n",
    "    \"\"\"Generates a series of bip bip at the given frequency.\"\"\"\n",
    "    t = torch.arange(\n",
    "        int(duration * sample_rate), device=\"cuda\", dtype=torch.float) / sample_rate\n",
    "    wav = torch.cos(2 * math.pi * frequency * t)[None]\n",
    "    tp = (t % (2 * bip_duration)) / (2 * bip_duration)\n",
    "    envelope = (tp >= 0.5).float()\n",
    "    return wav * envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd26c1d-4368-4838-bd2a-392717247366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use a synthetic signal to prompt the generated audio.\n",
    "res = model.generate_continuation(\n",
    "    get_bip_bip(0.125).expand(2, -1, -1), \n",
    "    16000, ['Whistling with wind blowing', \n",
    "            'Typing on a typewriter'], \n",
    "    progress=True)\n",
    "display_audio(res, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4041a-e164-455d-b366-e661ad880e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use any audio from a file. Make sure to trim the file if it is too long!\n",
    "prompt_waveform, prompt_sr = torchaudio.load(\"Yun Hi Chala Chal.mp3\")\n",
    "# prompt_waveform, prompt_sr = torchaudio.load(\"../assets/sirens_and_a_humming_engine_approach_and_pass.mp3\")\n",
    "\n",
    "prompt_duration = 6\n",
    "prompt_waveform = prompt_waveform[..., :int(prompt_duration * prompt_sr)]\n",
    "output = model.generate_continuation(prompt_waveform, prompt_sample_rate=prompt_sr, progress=True)\n",
    "display_audio(output, sample_rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fdf190-49bd-4ecd-9d13-afc54cf0a2b9",
   "metadata": {},
   "source": [
    "### Text-conditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5060c5c7-22bc-48b9-9c80-f93d74e322cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "output = model.generate(\n",
    "    descriptions=[\n",
    "        'Ar Rahman music styled tabla',\n",
    "        # 'Subway train blowing its horn',\n",
    "        'A cat meowing',\n",
    "    ],\n",
    "    progress=True\n",
    ")\n",
    "display_audio(output, sample_rate=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fb2439-a854-47ea-a149-bfbe76f09f50",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MAGNeT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ad9a1-ab5e-448a-a485-fed5d4d5b3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.models import MAGNeT\n",
    "\n",
    "model = MAGNeT.get_pretrained('facebook/magnet-small-10secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a9599-5e87-49cc-b2ee-4cd0c8211da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    temperature=3.0,\n",
    "    max_cfg_coef=10.0,\n",
    "    min_cfg_coef=1.0,\n",
    "    decoding_steps=[int(20 * model.lm.cfg.dataset.segment_duration // 10),  10, 10, 10],\n",
    "    span_arrangement='stride1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1211b5e9-14b9-4a01-83b2-20c07a0ac0c0",
   "metadata": {},
   "source": [
    "### Text-conditional Generation - Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39209539-eb4d-4cf2-a057-db032c196830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "###### Text-to-music prompts - examples ######\n",
    "text = \"80s bollywood music with deep instrumentals and base, hindi lyrics and very existential meaning\"\n",
    "# text = \"80s electronic track with melodic synthesizers, catchy beat and groovy bass. 170 bpm\"\n",
    "# text = \"Earthy tones, environmentally conscious, ukulele-infused, harmonic, breezy, easygoing, organic instrumentation, gentle grooves\"\n",
    "# text = \"Funky groove with electric piano playing blue chords rhythmically\"\n",
    "# text = \"Rock with saturated guitars, a heavy bass line and crazy drum break and fills.\"\n",
    "# text = \"A grand orchestral arrangement with thunderous percussion, epic brass fanfares, and soaring strings, creating a cinematic atmosphere fit for a heroic battle\"\n",
    "                   \n",
    "N_VARIATIONS = 3\n",
    "descriptions = [text for _ in range(N_VARIATIONS)]\n",
    "\n",
    "print(f\"text prompt: {text}\\n\")\n",
    "output = model.generate(descriptions=descriptions, progress=True, return_tokens=True)\n",
    "display_audio(output[0], sample_rate=model.compression_model.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b0bc6-7d14-42a5-aa62-82a553ca9ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text-conditional Generation - Sound Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12efd8ff-fca8-4803-9f16-a1057e0766fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.models import MAGNeT\n",
    "\n",
    "model = MAGNeT.get_pretrained('facebook/audio-magnet-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b7c6d-c481-446b-bb0e-d918759276c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=0,\n",
    "    top_p=0.8,\n",
    "    temperature=3.5,\n",
    "    max_cfg_coef=20.0,\n",
    "    min_cfg_coef=1.0,\n",
    "    decoding_steps=[int(20 * model.lm.cfg.dataset.segment_duration // 10),  10, 10, 10],\n",
    "    span_arrangement='stride1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f7960-caf8-4155-abe2-82b8751f7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.utils.notebook import display_audio\n",
    "               \n",
    "###### Text-to-audio prompts - examples ######\n",
    "text = \"Seagulls squawking as ocean waves crash while wind blows heavily into a microphone.\"\n",
    "# text = \"A toilet flushing as music is playing and a man is singing in the distance.\"\n",
    "\n",
    "N_VARIATIONS = 3\n",
    "descriptions = [text for _ in range(N_VARIATIONS)]\n",
    "\n",
    "print(f\"text prompt: {text}\\n\")\n",
    "output = model.generate(descriptions=descriptions, progress=True, return_tokens=True)\n",
    "display_audio(output[0], sample_rate=model.compression_model.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e88f1-b207-4e3e-b632-00fda63e401b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MusicGen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9f9b2-994a-4172-8ec0-14c30faa3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.models import MusicGen\n",
    "from audiocraft.models import MultiBandDiffusion\n",
    "\n",
    "USE_DIFFUSION_DECODER = True\n",
    "# Using small model, better results would be obtained with `medium` or `large`.\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-small')\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    mbd = MultiBandDiffusion.get_mbd_musicgen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4dd1ff-e4c4-4bac-923b-da668e9da2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_generation_params(\n",
    "    use_sampling=True,\n",
    "    top_k=250,\n",
    "    duration=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345572e-9378-461a-ab60-da6a5bf6b2aa",
   "metadata": {},
   "source": [
    "### Music Continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527893fb-51db-4264-8d0b-7ae0a655398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torchaudio\n",
    "import torch\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "def get_bip_bip(bip_duration=0.125, frequency=440,\n",
    "                duration=0.5, sample_rate=32000, device=\"cuda\"):\n",
    "    \"\"\"Generates a series of bip bip at the given frequency.\"\"\"\n",
    "    t = torch.arange(\n",
    "        int(duration * sample_rate), device=\"cuda\", dtype=torch.float) / sample_rate\n",
    "    wav = torch.cos(2 * math.pi * 440 * t)[None]\n",
    "    tp = (t % (2 * bip_duration)) / (2 * bip_duration)\n",
    "    envelope = (tp >= 0.5).float()\n",
    "    return wav * envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815d241e-7cdb-4140-b87c-47d680d57def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use a synthetic signal to prompt both the tonality and the BPM\n",
    "# of the generated audio.\n",
    "res = model.generate_continuation(\n",
    "    get_bip_bip(0.125).expand(2, -1, -1), \n",
    "    32000, ['Jazz jazz and only jazz', \n",
    "            'Hindi tabla with flute and sitar',\n",
    "            ], \n",
    "    progress=True)\n",
    "display_audio(res, 32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215becb-235f-4d96-858a-aa932360df2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use any audio from a file. Make sure to trim the file if it is too long!\n",
    "# prompt_waveform, prompt_sr = torchaudio.load(\"../assets/bach.mp3\")\n",
    "prompt_waveform, prompt_sr = torchaudio.load(\"Yun Hi Chala Chal.mp3\")\n",
    "prompt_duration = 8\n",
    "prompt_waveform = prompt_waveform[..., :int(prompt_duration * prompt_sr)]\n",
    "output = model.generate_continuation(prompt_waveform, prompt_sample_rate=prompt_sr, progress=True, return_tokens=True)\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b14b2c1-df02-46bb-925a-f3a0c2f402ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "output = model.generate(\n",
    "    descriptions=[\n",
    "        #'80s pop track with bassy drums and synth',\n",
    "        #'90s rock song with loud guitars and heavy drums',\n",
    "        #'Progressive rock drum and bass solo',\n",
    "        #'Punk Rock song with loud drum and power guitar',\n",
    "        #'Bluesy guitar instrumental with soulful licks and a driving rhythm section',\n",
    "        #'Jazz Funk song with slap bass and powerful saxophone',\n",
    "        # 'drum and bass beat with intense percussions',\n",
    "        'Hindi tabla with flute and sitar'\n",
    "    ],\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b86e7d-86c8-4013-99c9-50662790d7b8",
   "metadata": {},
   "source": [
    "### Melody-conditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ecd97c-1c52-4d82-b67e-03bffe9c6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from audiocraft.utils.notebook import display_audio\n",
    "\n",
    "model = MusicGen.get_pretrained('facebook/musicgen-melody')\n",
    "model.set_generation_params(duration=8)\n",
    "\n",
    "melody_waveform, sr = torchaudio.load(\"Yun Hi Chala Chal.mp3\")\n",
    "melody_waveform = melody_waveform.unsqueeze(0).repeat(2, 1, 1)\n",
    "output = model.generate_with_chroma(\n",
    "    descriptions=[\n",
    "        # '80s pop track with bassy drums and synth',\n",
    "        '90s bollywood music with deep instrumentals and base, hindi lyrics and very existential meaning',\n",
    "        'Hindi tabla with flute and sitar'\n",
    "    ],\n",
    "    melody_wavs=melody_waveform,\n",
    "    melody_sample_rate=sr,\n",
    "    progress=True, return_tokens=True\n",
    ")\n",
    "display_audio(output[0], sample_rate=32000)\n",
    "if USE_DIFFUSION_DECODER:\n",
    "    out_diffusion = mbd.tokens_to_wav(output[1])\n",
    "    display_audio(out_diffusion, sample_rate=32000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a8f21-b5b6-418e-932b-ff17b19ff585",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc1b231-cf97-4217-b392-6a77a9c0ba3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'prompt_golden_data.csv'\n",
    "\n",
    "\n",
    "def csv_to_json(csv_file_path, json_file_path):\n",
    "    data = []\n",
    "\n",
    "    # Read the CSV file\n",
    "    with open(csv_file_path, 'r') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "\n",
    "        # Iterate over each row in the CSV file\n",
    "        for row in csv_reader:\n",
    "            data.append(row)\n",
    "\n",
    "    # Write the data to a JSON file\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "    print(f\"CSV file '{csv_file_path}' has been converted to JSON file '{json_file_path}'.\")\n",
    "\n",
    "# Example usage\n",
    "csv_file_path = 'prompt_golden_data.csv'\n",
    "json_file_path = 'music_lyrics.json'\n",
    "csv_to_json(csv_file_path, json_file_path)\n",
    "# Step 1: Load the original JSON data\n",
    "with open('music_lyrics.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Step 2: Modify the data structure\n",
    "new_data = []\n",
    "\n",
    "for song in data:\n",
    "    new_data.append({\n",
    "        'instruction': 'Write a song lyric based on the given inputs and verse prompt.',\n",
    "        'input': \n",
    "            'genres:'+ song['genres']+\n",
    "            'progression:'+ song['progression']+\n",
    "            'start_key'+ song['start_key']+\n",
    "            'verse prompt'+ song['prompts']\n",
    "        ,\n",
    "        'output': \n",
    "            'lyrics'+ song['processed_lyrics']  # Ensure this key 'lyrics' or similar is what you want\n",
    "        \n",
    "    })\n",
    "\n",
    "# Step 3: Write the modified data to a new JSON file\n",
    "with open('restructured.json', 'w') as file:\n",
    "    json.dump(new_data, file, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "with open('data/dataset_info.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    data.append('music_lyrics_generation')\n",
    "    data['music_lyrics_generation']['file_name']='music_lyrics_generation.json'\n",
    "    data['music_lyrics_generation']['file_sha1']='7df69e4325ad88feef052b3c086b4434867b120a'\n",
    "    json.dump(data, file, indent=4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6b5876-bffb-42d3-8e64-32d4bb605396",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Create melody of generated lyrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ecbd0-895f-4332-8d1c-b4a30c9e04a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import yaml\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import copy\n",
    "import re\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# ckpt_path = '/Mar2Ding/songcomposer_pretrain'\n",
    "ckpt_path = 'Mar2Ding/songcomposer_sft'\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt_path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(ckpt_path, trust_remote_code=True).cuda().half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fab8d8-3c46-48da-9252-935853059273",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Compose a tune in harmony with the accompanying lyrics. <bol> Total 6 lines.\\\n",
    "The first line: NLP, the class that sets us free\\n\\\n",
    "The second line: Prof. Srihari and Sayantal, our guiding team\\n\\\n",
    "The third line: Natural Language Processing, oh so fine They want to grade us high, all the time\\n\\\n",
    "The fourth line: Parsing, disambiguating, we’re on a roll NLP, you’re in our soul\\n\\\n",
    "The fifth line: Natural Language Processing, oh so fine They want to grade us high, all the time\\n\\\n",
    "The sixth line: Prof. Srihari, Sayantal Pal, we thank you NLP, our passion true\\n<eol>'\n",
    "####### m2l #######\n",
    "model.inference(prompt, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31147ff-f3f7-42a6-aedb-3ddc11ead0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "line = 'The first line:<E4> , <154> , <88> |<E4> , <134> , <88> |<E4> , <137> , <79> |<F#4> , <151> , <79> |<E4> , <154> , <79> |<D#4> , <154> , <79> | <C#4> , <157> , <79> | <B3> , <172> , <127> The second line:<E4> , <151> , <88> |<E4> , <137> , <88> |<E4> , <137> , <79> |<F#4> , <151> , <79> |<E4> , <151> , <79> |<D#4> , <160> , <79> |<C#4> , <157> , <79> The third line:<B3> , <151> , <79> |<G#3> , <137> , <79> |<B3> , <151> , <79> |<G#3> , <189> , <79> |<F#3> , <157> , <79> |<G#3> , <137> , <79> The fourth line:<G#3> , <147> , <79> |<F#3> , <144> , <79> |<E3> , <151> , <79> |<F#3> , <141> , <79> |<G#3> , <166> , <79> |<B3> , <219> , <160> The fifth line:<E4> , <154> , <88> |<E4> , <130> , <88> |<E4> , <144> , <79> |<F#4> , <147> , <79> |<E4> , <157> , <79> |<D#4> , <154> , <79> |<C#4> , <151> , <79> |<B3> , <118> , <79> |<B3> , <118> , <79> |<G#3> , <207> , <79> |<B3> , <205> , <79> |的, <G#3> , <205> , <79>'\n",
    "from finetune.utils import gen_midi\n",
    "gen_midi(line, 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853b2019-1491-492f-b6df-1a17ef2dc0c9",
   "metadata": {},
   "source": [
    "# Finetune LLama 3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005f0be-6426-4c7c-893e-34ee8d40a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/\n",
    "%rm -rf LLaMA-Factory\n",
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "%cd LLaMA-Factory\n",
    "%ls\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers==0.0.25\n",
    "!pip install .[bitsandbytes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d633a6a-7240-47ea-866a-7ff74b02630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "try:\n",
    "  assert torch.cuda.is_available() is True\n",
    "except AssertionError:\n",
    "  print(\"Please set up a GPU before using LLaMA Factory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f42ba82-7e5a-40ae-afe7-fa759ce13c56",
   "metadata": {},
   "source": [
    "## Fine-tune model via Command Line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb79abbb-73bc-4616-b10d-f6ce94d3921c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/LLaMA-Factory/\n",
    "!GRADIO_SHARE=1 llamafactory-cli webui\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "args = dict(\n",
    "  stage=\"sft\",                        # do supervised fine-tuning\n",
    "  do_train=True,\n",
    "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  dataset=\"identity,alpaca_gpt4_en,\",             # use alpaca and identity datasets\n",
    "  template=\"llama3\",                     # use llama3 prompt template\n",
    "  finetuning_type=\"lora\",                   # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
    "  output_dir=\"llama3_lora\",                  # the path to save LoRA adapters\n",
    "  per_device_train_batch_size=2,               # the batch size\n",
    "  gradient_accumulation_steps=4,               # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",                 # use cosine learning rate scheduler\n",
    "  logging_steps=10,                      # log every 10 steps\n",
    "  warmup_ratio=0.1,                      # use warmup scheduler\n",
    "  save_steps=1000,                      # save checkpoint every 1000 steps\n",
    "  learning_rate=5e-5,                     # the learning rate\n",
    "  num_train_epochs=3.0,                    # the epochs of training\n",
    "  max_samples=500,                      # use 500 examples in each dataset\n",
    "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
    "  quantization_bit=4,                     # use 4-bit QLoRA\n",
    "  loraplus_lr_ratio=16.0,                   # use LoRA+ algorithm with lambda=16.0\n",
    "  use_unsloth=True,                      # use UnslothAI's LoRA optimization for 2x faster training\n",
    "  fp16=True,                         # use float16 mixed precision training\n",
    ")\n",
    "\n",
    "json.dump(args, open(\"train_llama3.json\", \"w\", encoding=\"utf-8\"), indent=2)\n",
    "\n",
    "%cd /content/LLaMA-Factory/\n",
    "\n",
    "!llamafactory-cli train train_llama3.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463f4c2c-be33-4690-9c70-39e9f8606799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open(\"data/music_lyrics_generation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "  dataset = json.load(f)\n",
    "  print(dataset[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd123326-b697-4c01-a2d4-b28d212eadb8",
   "metadata": {},
   "source": [
    "### Infer the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebafb40-ce24-4e70-b879-d2b63516d0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtuner.chat import ChatModel\n",
    "from llmtuner.extras.misc import torch_gc\n",
    "\n",
    "%cd /content/LLaMA-Factory/\n",
    "\n",
    "args = dict(\n",
    "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
    "  template=\"llama3\",                     # same to the one in training\n",
    "  finetuning_type=\"lora\",                  # same to the one in training\n",
    "  quantization_bit=4,                    # load 4-bit quantized model\n",
    "  use_unsloth=True,                     # use UnslothAI's LoRA optimization for 2x faster generation\n",
    ")\n",
    "chat_model = ChatModel(args)\n",
    "\n",
    "messages = []\n",
    "print(\"Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\")\n",
    "while True:\n",
    "  query = input(\"\\nUser: \")\n",
    "  if query.strip() == \"exit\":\n",
    "    break\n",
    "  if query.strip() == \"clear\":\n",
    "    messages = []\n",
    "    torch_gc()\n",
    "    print(\"History has been removed.\")\n",
    "    continue\n",
    "\n",
    "  messages.append({\"role\": \"user\", \"content\": query})\n",
    "  print(\"Assistant: \", end=\"\", flush=True)\n",
    "\n",
    "  response = \"\"\n",
    "  for new_text in chat_model.stream_chat(messages):\n",
    "    print(new_text, end=\"\", flush=True)\n",
    "    response += new_text\n",
    "  print()\n",
    "  messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "torch_gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c478dbf7-5183-4aae-aeab-030471b4f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtuner import ChatModel\n",
    "from llmtuner.extras.misc import torch_gc\n",
    "\n",
    "\n",
    "chat_model = ChatModel(dict(\n",
    "  model_name_or_path=\"unsloth/llama-3-8b-Instruct-bnb-4bit\", # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  adapter_name_or_path=\"llama3_lora\",            # load the saved LoRA adapters\n",
    "  finetuning_type=\"lora\",                  # same to the one in training\n",
    "  template=\"llama3\",                     # same to the one in training\n",
    "  quantization_bit=4,                    # load 4-bit quantized model\n",
    "  use_unsloth=True,                     # use UnslothAI's LoRA optimization for 2x faster generation\n",
    "))\n",
    "\n",
    "messages = []\n",
    "while True:\n",
    "  query = input(\"\\nUser: \")\n",
    "  if query.strip() == \"exit\":\n",
    "    break\n",
    "\n",
    "  if query.strip() == \"clear\":\n",
    "    messages = []\n",
    "    torch_gc()\n",
    "    print(\"History has been removed.\")\n",
    "    continue\n",
    "\n",
    "    \n",
    "\n",
    "    query = \"'instruction': 'Write a song lyric based on the given inputs and verse prompt.'\n",
    "\n",
    "    'user': 'genres': ['canadian pop', 'pop', 'post-teen pop']\n",
    "            'progression':['A', 'Em', 'G']\n",
    "            'start_key': 'Bm'\n",
    "            'verse prompt': 'I was fifteen when the world put me on a pedestal and told me Im the best.'\n",
    "    \"\n",
    "    messages.append({\"role\": \"user\", \"content\": query})     # add query to messages\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "    response = \"\"\n",
    "    for new_text in chat_model.stream_chat(messages):      # stream generation\n",
    "      print(new_text, end=\"\", flush=True)\n",
    "      response += new_text\n",
    "    print()\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response}) # add response to messages\n",
    "\n",
    "torch_gc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "song_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
